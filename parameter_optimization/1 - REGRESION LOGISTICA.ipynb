{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de parámetros para Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6435823, 19)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_work_3 = pd.read_csv('D:\\DataSet\\dataset_work_transformed.csv')\n",
    "x = dataset_work_3.iloc[:,:-1]\n",
    "y = dataset_work_3.iloc[:,-1].values\n",
    "dataset_work_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=1438.44988828766, max_iter=100, penalty=None, solver=liblinear; total time=   2.0s\n",
      "[CV] END C=0.08858667904100823, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.8s\n",
      "[CV] END C=0.0018329807108324356, max_iter=100, penalty=elasticnet, solver=saga; total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=1438.44988828766, max_iter=100, penalty=None, solver=liblinear; total time=   2.0s\n",
      "[CV] END C=0.08858667904100823, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.8s\n",
      "[CV] END C=3792.690190732246, max_iter=200, penalty=l2, solver=liblinear; total time= 1.4min\n",
      "[CV] END C=545.5594781168514, max_iter=200, penalty=l2, solver=liblinear; total time= 1.5min\n",
      "[CV] END C=545.5594781168514, max_iter=200, penalty=l2, solver=liblinear; total time= 1.7min\n",
      "[CV] END C=4.281332398719396, max_iter=200, penalty=elasticnet, solver=saga; total time=   1.9s\n",
      "[CV] END C=4.281332398719396, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.7s\n",
      "[CV] END C=0.0018329807108324356, max_iter=100, penalty=elasticnet, solver=saga; total time=   0.6s\n",
      "[CV] END C=3792.690190732246, max_iter=200, penalty=l2, solver=liblinear; total time= 1.6min\n",
      "[CV] END C=4.281332398719396, max_iter=200, penalty=elasticnet, solver=saga; total time=   1.8s\n",
      "[CV] END C=4.281332398719396, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.7s\n",
      "[CV] END C=0.0018329807108324356, max_iter=100, penalty=elasticnet, solver=saga; total time=   0.6s\n",
      "[CV] END C=3792.690190732246, max_iter=200, penalty=l2, solver=liblinear; total time= 1.6min\n",
      "[CV] END C=545.5594781168514, max_iter=200, penalty=l2, solver=liblinear; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END C=0.0018329807108324356, max_iter=100, penalty=l1, solver=saga; total time= 5.4min\n",
      "[CV] END C=0.0018329807108324356, max_iter=100, penalty=l1, solver=saga; total time= 5.6min\n",
      "[CV] END C=0.0018329807108324356, max_iter=100, penalty=l1, solver=saga; total time= 5.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 80, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1204, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 75, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.87872862        nan 0.88113423        nan 0.8801736         nan\n",
      "        nan 0.8801736         nan 0.88113412]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados:\n",
      "{'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 200, 'C': 545.5594781168514}\n",
      "[CV] END C=545.5594781168514, max_iter=200, penalty=None, solver=saga; total time= 8.0min\n",
      "[CV] END C=0.08858667904100823, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   1.4s\n",
      "[CV] END C=206.913808111479, max_iter=200, penalty=None, solver=saga; total time= 8.7min\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[837397 283442]\n",
      " [130961 678947]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80   1120839\n",
      "           1       0.71      0.84      0.77    809908\n",
      "\n",
      "    accuracy                           0.79   1930747\n",
      "   macro avg       0.79      0.79      0.78   1930747\n",
      "weighted avg       0.80      0.79      0.79   1930747\n",
      "\n",
      "ROC AUC: 0.88\n",
      "El tiempo total de ejecución fue de 15 minutos y 8.79 segundos.\n",
      "[CV] END C=1438.44988828766, max_iter=100, penalty=None, solver=liblinear; total time=   1.6s\n",
      "[CV] END C=4.281332398719396, max_iter=100, penalty=elasticnet, solver=liblinear; total time=   0.7s\n",
      "[CV] END C=206.913808111479, max_iter=200, penalty=None, solver=saga; total time=10.1min\n",
      "[CV] END C=545.5594781168514, max_iter=200, penalty=None, solver=saga; total time=10.1min\n",
      "[CV] END C=4.281332398719396, max_iter=200, penalty=elasticnet, solver=saga; total time=   1.7s\n",
      "[CV] END C=206.913808111479, max_iter=200, penalty=None, solver=saga; total time=10.2min\n",
      "[CV] END C=545.5594781168514, max_iter=200, penalty=None, solver=saga; total time=10.8min\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Registro del tiempo de inicio\n",
    "start_time = time.time()\n",
    "\n",
    "# 1. Utilizar el 100% del dataset\n",
    "X = dataset_work_3.drop(columns=[\"MOROSIDAD\"])\n",
    "y = dataset_work_3[\"MOROSIDAD\"]\n",
    "\n",
    "# 2. Escalado de características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. División del conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 4. Espacio de búsqueda para los hiperparámetros\n",
    "param_dist = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# 5. Inicializar el clasificador y RandomizedSearchCV\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Número de combinaciones a probar\n",
    "    cv=3,  # Validación cruzada con 3 folds\n",
    "    n_jobs=-1,  # Paralelismo\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "# 6. Ajustar el modelo con el dataset completo\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# 7. Obtener los mejores parámetros\n",
    "print(\"Mejores parámetros encontrados:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# 8. Entrenar el modelo final con todos los datos utilizando los mejores parámetros\n",
    "best_params = random_search.best_params_\n",
    "best_model = LogisticRegression(**best_params, random_state=42)\n",
    "\n",
    "# Ajustar el modelo con los datos completos de entrenamiento\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Medir el tiempo de finalización y calcular el tiempo transcurrido\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convertir a minutos y segundos\n",
    "minutes = elapsed_time // 60\n",
    "seconds = elapsed_time % 60\n",
    "\n",
    "print(f\"El tiempo total de ejecución fue de {int(minutes)} minutos y {seconds:.2f} segundos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
