{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización de parámetros con LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6435823, 19)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_work_3 = pd.read_csv('D:\\DataSet\\dataset_work_transformed.csv')\n",
    "x = dataset_work_3.iloc[:,:-1]\n",
    "y = dataset_work_3.iloc[:,-1].values\n",
    "dataset_work_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.25.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from lightgbm) (1.11.4)\n",
      "Using cached lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.411683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2863\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=50, subsample=0.8; total time=52.5min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.425071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2860\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=10, min_child_samples=10, n_estimators=200, num_leaves=50, subsample=0.8; total time=52.5min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.442956 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2863\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=-1, min_child_samples=30, n_estimators=100, num_leaves=31, subsample=1.0; total time=22.9min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.375913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2863\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=20, subsample=1.0; total time=30.9min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.339184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2863\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=50, subsample=1.0; total time=53.8min\n",
      "[LightGBM] [Info] Number of positive: 1259590, number of negative: 1743794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.467977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2856\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419390 -> initscore=-0.325277\n",
      "[LightGBM] [Info] Start training from score -0.325277\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=-1, min_child_samples=30, n_estimators=100, num_leaves=31, subsample=1.0; total time=22.9min\n",
      "[LightGBM] [Info] Number of positive: 1259590, number of negative: 1743794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.747955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2856\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419390 -> initscore=-0.325277\n",
      "[LightGBM] [Info] Start training from score -0.325277\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=-1, min_child_samples=30, n_estimators=200, num_leaves=20, subsample=1.0; total time=31.0min\n",
      "[LightGBM] [Info] Number of positive: 1259590, number of negative: 1743794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.385309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2856\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419390 -> initscore=-0.325277\n",
      "[LightGBM] [Info] Start training from score -0.325277\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=50, subsample=1.0; total time=54.4min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.337203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2860\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=20, min_child_samples=30, n_estimators=200, num_leaves=50, subsample=1.0; total time=54.7min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.406234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2863\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50, subsample=1.0; total time=27.8min\n",
      "[LightGBM] [Info] Number of positive: 1259591, number of negative: 1743793\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.304738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2860\n",
      "[LightGBM] [Info] Number of data points in the train set: 3003384, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419391 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=20, min_child_samples=10, n_estimators=100, num_leaves=50, subsample=0.8; total time=27.2min\n",
      "[LightGBM] [Info] Number of positive: 1889386, number of negative: 2615690\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.314999 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2848\n",
      "[LightGBM] [Info] Number of data points in the train set: 4505076, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419390 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "Mejores parámetros encontrados: {'subsample': 1.0, 'num_leaves': 50, 'n_estimators': 200, 'min_child_samples': 30, 'max_depth': 20, 'learning_rate': 0.2, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Info] Number of positive: 1889386, number of negative: 2615690\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.307654 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2848\n",
      "[LightGBM] [Info] Number of data points in the train set: 4505076, number of used features: 18\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.419390 -> initscore=-0.325276\n",
      "[LightGBM] [Info] Start training from score -0.325276\n",
      "\n",
      "Matriz de Confusión:\n",
      "[[900366 220473]\n",
      " [133745 676163]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.84   1120839\n",
      "           1       0.75      0.83      0.79    809908\n",
      "\n",
      "    accuracy                           0.82   1930747\n",
      "   macro avg       0.81      0.82      0.81   1930747\n",
      "weighted avg       0.82      0.82      0.82   1930747\n",
      "\n",
      "ROC AUC: 0.92\n",
      "El tiempo total de ejecución fue de 62 minutos y 22.46 segundos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Registro del tiempo de inicio\n",
    "start_time = time.time()\n",
    "\n",
    "# Cargar dataset\n",
    "X = dataset_work_3.drop(columns=[\"MOROSIDAD\"])\n",
    "y = dataset_work_3[\"MOROSIDAD\"]\n",
    "\n",
    "# Escalado de características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# División del conjunto de datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Definición del espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'num_leaves': [20, 31, 50],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Inicializar el modelo y RandomizedSearchCV\n",
    "lgbm = LGBMClassifier(random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Número de combinaciones a probar\n",
    "    cv=3,  # Validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Entrenar el modelo con la búsqueda de hiperparámetros\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = random_search.best_params_\n",
    "print(\"Mejores parámetros encontrados:\", best_params)\n",
    "\n",
    "# Entrenar el modelo final con los mejores parámetros\n",
    "best_model = LGBMClassifier(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones y evaluación\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nMatriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Registro del tiempo de finalización\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Convertir a minutos y segundos\n",
    "minutes = elapsed_time // 60\n",
    "seconds = elapsed_time % 60\n",
    "print(f\"El tiempo total de ejecución fue de {int(minutes)} minutos y {seconds:.2f} segundos.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
